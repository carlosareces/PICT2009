
%\fixme{C: This intro looks out of place.}
%Given the nature of the interaction we model, the system must
%be capable of adapting to possible errors of interpretation
%or execution of the instructions, giving corrective instructions when necessary.
%Repairing misinterpretations is challenging, we will face this challenge by
%designing our system in successive stages of increasing complexity.
%
%Initially, the flow of linguistic information will be one-way, from the
%system to the user, so the user can not ask for any help. At later stages of the
%project, the unidirectional model will be extended to allow for bidirectional
%language exchange: for example, the user may request clarifications or redefine
%the objective to be achieved using natural language.
%
%The generation of instructions for a specific domain requires linguistic
%information at different levels. At the morphological and syntactic level a
%grammar of the chosen language (Spanish) is required. At the semantic
%level a repository of lexical information organized ontologically is needed. At
%the pragmatic level, we need a formal description of the possible actions in the
%domain (with their preconditions and effects), as well as information about the
%state of the task.

%This domain dependent information is used by various components of the
%dialogue system that manages the user interaction. In a typical system, this is
%as detailed below. In the direction system $\rightarrow$ user, the component
%responsible for \emph{content planning} generates a sequence of instructions
%relevant to the current moment of the interaction, then the \emph{knowledge
%management} component updates the contextual information and, finally, the
%component in charge of \emph{generating the instruction} transmits the 
%instructions as natural language expressions. In the
%opposite direction, i.e. user $\rightarrow$ system, the component called
%\emph{discretized} transforms the continuous flow of information obtained from
%user behavior in discrete actions relevant to the task at hand, while the
%components of content planning and knowledge management are concerned with
%managing the current context and detecting possible errors.

%\subsection{Main Tasks of the Project}

% We will use the platform developed as part of
% the challenge \emph{GIVE, Generating Instructions in Virtual
% Environments}\footnote{\url{http://www.give-challenge.org}}~\cite{byron09}
% which provides a basic
% platform for this type of system.
% The main aim of GIVE is to act as an evaluation framework for
% natural language generation systems. 
%In the GIVE Challenge the systems are
%evaluated homogeneously since all the participating systems use the same
%resources provided by the organizers. 

% The GIVE platform is thus an appropriate tool for starting to investigate
% various aspects of an interaction situated in a virtual environment: the
% problem
% of
% morpho-syntactic realization of the instructions, knowledge representation of
% the context of interaction (discourse context, ontologies, possible actions),
% the type of inference required by the tasks involved (planning, model building
% and model checking), and the pragmatic rules of interaction required by the
% task
% (managing the cognitive load, update and use of contextual information).
% Moreover, 

For the correct definition of the interaction policies of our prototype we need
a corpus that provides examples of typical interactions in the domain. The GIVE
platform provides tools for collecting such corpus, namely a Wizard of Oz
platform for collecting interactions in which two persons interact to solve a
task in the GIVE domain. In the scenario proposed by GIVE, a
human user carries out a task of ``treasure hunt'' in a 3D virtual environment
and the task of the generation system is to provide real-time, natural
language instructions that help the user find the hidden treasure. GIVE
provides tools to record all details of the interaction, thus
allowing to easily obtain a corpus of interaction in virtual environments
annotated automatically.

From the collected corpus we will begin the design, implementation and testing
of our dialogue system. There are traditionally four tasks
that a dialogue system must implement: (1) planning the content, (2)
generating of referring expressions, (3) managing the interaction context, and
(4) interpretation of user responses. 

\emph{(1) Content planning:} The first step is to obtain a plan to reach the
task goal from the current state.
The plan will contain physical actions on the virtual environment. The second
step is to decide how to transmit this sequence of actions. That is, to decide
how many actions to communicate per instruction and how to aggregate them
coherently. The result of the action aggregation process can be represented as a
tree describing the task structure at different levels of abstraction. The third
and final step is to decide how to navigate the tree of actions to verbalize the
instructions (for example, post or preorder as
explored in~\cite{foster-etal-ijcai2009}). This project will investigate
different aggregation policies (e.g., aggregating actions that
manipulate similar objects) and non-standard policies of navigation of the tree
(e.g., moving to a lower level of abstraction in case of misunderstandings).

In order to get the plan we use the inference task of planning~\cite{nau04}.
GIVE provides a very limited planner. The state of the art in the planning area
does not cover the requirements of our
system. While there are systems that work well when optimized for certain
applications, none provides services such as the generation of alternative
plans, or the generation of incomplete plans in case of the absence of plan.
Hence, we will have to design and implement these extensions to the planning
algorithms. We will also study the theoretical behavior (e.g., complexity) of
these new algorithms. 

\emph{(2) Generating referring expressions:} Once the content planning is
complete, the next task is the generation of referring expressions. This task
involves
producing a phrase that describes a referable entity so that the user can
identify it (e.g., ``the vase on the table''). To be
acceptable, these expressions should not be too heavy cognitively (for
example, ``the vase that is not above the chair or sofa or under the
table'' would not be acceptable). In~\cite{AKS08} the authors propose to
do the symbolic minimization model that represents the state of the world, in
order to obtain logical formulas that describe each object uniquely. In our
project we
will implement this method and evaluate it within the dialogue system.

\emph{(3) Management of the Interaction Context:} To manage the use of
interaction context we will use existing knowledge maintenance systems such as
RACER\footnote{\url{http://www.racer-systems.com}} or Pellet\footnote{\url{http://clarkparsia.com/pellet}}, which support tasks of
definition, maintenance and querying of ontologies. These systems have been used
as inference engines in numerous applications in
the area and, in particular,
in applications designed by us~\cite{benotti09b}. Once we
have studied the behavior of these inference engines on the task, we will
analyze
its limitations and investigate the required extensions.

\emph{(4) Interpretation of User Responses:} The interpretation of user
responses in the unidirectional system is relatively simple; it ammounts to
discretizing the continuos flow of user behaviour in the 3D world into actions
meaningful for the domain task. In a first
stage, we will use the discretizer provided by GIVE. After evaluating it we can
determine whether or not this module meets the requirements of
our task and what are its limitations. In the bidirectional system, however,
the interpretation of user responses is the task that will require more
attention.

To start with, the bidirectional system should be expanded with capabilities
for  processing statements coming from the user (namely, parsing, semantic
construction, resolution of references, etc.). We will study, in particular, two
types of user contributions: requests for clarification of the instruction
given (that we call `short-term repairs'), and redefinition of goals (that we
call `long-term repairs'). We will implement short-term repairs
using Purver~\shortcite{purver06}'s approach. For long-term repairs will use the
guidelines of~\cite{blaylock05a}. 
\medskip

\subsection{Evaluation}
To determine the quality of the obtained propotypes we propose to study the
creation of a model of quality assessment by the standards of software ISO/IEC
9126 and 14528~\cite{ISO9126-1,ISO14598-1}, which were successfully applied to
the MT domain, as shown in the tool FEMTI (Framework for the Evaluation of
Machine Translation)\footnote{\url{http://www.issco.unige.ch/femti/}}. FEMTI~\cite{Est2005}
try to guide the evaluators towards creating parameterized evaluation
plans that include various aspects of the system to evaluate and offer a
relevant set of metrics. The identification of relevant metrics can be performed
using various methods, e.g. based on previous
experience~\cite{paradise06,Chu2000,Litman2002}, conducting
surveys or requirement specifications (as in \cite{Lecoeuche98}) or
collecting such data through experiments called ``Wizard of Oz'' where the user
interacts with a prototype system (possibly incomplete or reduced in
functionality) and a human (the ``Wizard'') behind the interface responds as if
he were the system~\cite{Dahlback93,Fabbrizio05}.

After developing a quality model, several methodologies to assess
various aspects of the system can be applied: automatic metrics,
subjective metrics or metrics based on the task (both to
evaluate the contribution of each component as well as the quality of the whole
system). 

The GIVE platform is used every year as a unified framework for evaluating
generation systems. Systems that not only have to generate natural language
but also have to be able to participate in a real-time interaction situated in a
3D environment. The GIVE Challenge is one of the shared tasks endorsed by
ACL's special interests groups in generation, dialogue and semantics. We plan
to participate in the challenge, which will serve as an additional
source of information about aspects of the system that need
improvement.

Once the prototype is evaluated and improved using the results
of this evaluation, we will investigate its use as a virtual language tutor as
described in the next section.

\subsection{Applications}

The project outcome will be a system capable of giving natural language
instructions situated in a virtual 3D environment. The technology and
theoretical advances of the project can be used in various applications
(electronic commerce, technical support, voice control devices, etc.). 
Given the architecture of the system, you can change the language of interaction
with the system (input and output), just by changing the linguistic resources
for the appropriate language. During
the last year of the project will investigate its use for distance learning,
adapting the system to operate as a foreign language
tutor~\cite{Eskenazi09,Wik09}.



A one-way system that generates instructions in English can be used to test the
user understanding of a foreign language. The correct interpretation of the
instructions can be evaluated from the proper execution of the instructions. The
two-way system will allow the user to formulate clarifications (either in their
native language, or in the foreign language, if they want to practice
production in the foreign language). The user may also redefine the
objective to be achieved during the interaction, and thus select the type of
vocabulary he wants to practice.

Virtual worlds (like Second Life) are being rapidly incorporated into
education, both initial and superior~\cite{Doswell05,molk:lear09}. The use of a
virtual tutor has certain advantages over a human tutor.
Engwall~\shortcite{engwall1020} mentioned the following. (1) Amount of
practice: the chance to practice the new language is essential for learning, and
a virtual tutor provides opportunities only limited by the
technological resources. (2) Prestige: a student
may feel embarrassed about making mistakes with a human tutor, and this
might limit his willingness to speak in the foreign language. (3) Augmented
Reality: a virtual
tutor can provide additional material (e.g., examples in context, explanatory
images, etc.) with less effort than a human tutor.

To estimate the effectiveness of our system we will make a comparative
evaluation (with other existing tutoring systems) and a user-oriented evaluation
The main challenge of this study is to establish the most important features for
a language tutor and identify the relevant set of metrics. 








